{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "running -> run\n",
      "ran -> ran\n",
      "runs -> run\n",
      "easily -> easili\n",
      "fairly -> fairli\n",
      "\n",
      "Lancaster Stemmer:\n",
      "running -> run\n",
      "ran -> ran\n",
      "runs -> run\n",
      "easily -> easy\n",
      "fairly -> fair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Apply stemming\n",
    "print(\"Porter Stemmer:\")\n",
    "for word in words:\n",
    "    print(f\"{word} -> {porter.stem(word)}\")\n",
    "\n",
    "print(\"\\nLancaster Stemmer:\")\n",
    "for word in words:\n",
    "    print(f\"{word} -> {lancaster.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"\"\"\n",
    "Muad'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn.\n",
    "It's shocking to find how many people do not believe they can learn,\n",
    "and how many more believe learning to be difficult.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nMuad'Dib learned rapidly because his first training was in how to learn.\",\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Muad'Dib\",\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_quote = word_tokenize(worf_quote)\n",
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [word for word in words_in_quote if word.casefold() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "string_for_stemming = \"\"\"The crew of the USS Discovery discovered many discoveries.\n",
    "Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging Parts of Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagan_quote = \"\"\"\n",
    "If you wish to make an apple pie from scratch,you must first invent the universe.\"\"\"\n",
    "sagan_quote = \"\"\"Sahil is living in Surat\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sahil', 'is', 'living', 'in', 'Surat']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_sagan_quote = word_tokenize(sagan_quote)\n",
    "words_in_sagan_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sahil', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('living', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('Surat', 'NNP')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.pos_tag(words_in_sagan_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"scarves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n",
    "string_for_lemmatizing = \"\"\"The crew of the USS Discovery discovered many discoveries.\n",
    "Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'USS',\n",
       " 'Discovery',\n",
       " 'discovered',\n",
       " 'many',\n",
       " 'discoveries',\n",
       " '.',\n",
       " 'Discovering',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explorers',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'USS',\n",
       " 'Discovery',\n",
       " 'discovered',\n",
       " 'many',\n",
       " 'discovery',\n",
       " '.',\n",
       " 'Discovering',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explorer',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worst'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NE type\tExamples\n",
    "# ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
    "# PERSON\tEddy Bonte, President Obama\n",
    "# LOCATION\tMurray River, Mount Everest\n",
    "# DATE\tJune, 2008-06-29\n",
    "# TIME\ttwo fifty a m, 1:30 p.m.\n",
    "# MONEY\t175 million Canadian dollars, GBP 10.40\n",
    "# PERCENT\ttwenty pct, 18.75 %\n",
    "# FACILITY\tWashington Monument, Stonehenge\n",
    "# GPE\tSouth East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"\"\"\n",
    "Men like Schiaparelli watched the red planet—it is odd, by-the-bye, that\n",
    "for countless centuries Mars has been the star of war—but failed to\n",
    "interpret the fluctuating appearances of the markings they mapped so well.\n",
    "All that time the Martians must have been getting ready.\n",
    "During the opposition of 1894 a great light was seen on the illuminated\n",
    "part of the disk, first at the Lick Observatory, then by Perrotin of Nice,\n",
    "and then by other observers. English readers heard of it first in the\n",
    "issue of Nature dated August 2.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ne(quote):\n",
    "    words = word_tokenize(quote, language='english')\n",
    "    tags = nltk.pos_tag(words)\n",
    "    print(tags)\n",
    "    tree = nltk.ne_chunk(tags, binary=True)\n",
    "    # print(tree)\n",
    "    return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree\n",
    "        if hasattr(t, \"label\") and t.label() == \"NE\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Men', 'NNS'), ('like', 'IN'), ('Schiaparelli', 'NNP'), ('watched', 'VBD'), ('the', 'DT'), ('red', 'JJ'), ('planet—it', 'NN'), ('is', 'VBZ'), ('odd', 'JJ'), (',', ','), ('by-the-bye', 'JJ'), (',', ','), ('that', 'IN'), ('for', 'IN'), ('countless', 'JJ'), ('centuries', 'NNS'), ('Mars', 'NNP'), ('has', 'VBZ'), ('been', 'VBN'), ('the', 'DT'), ('star', 'NN'), ('of', 'IN'), ('war—but', 'NN'), ('failed', 'VBD'), ('to', 'TO'), ('interpret', 'VB'), ('the', 'DT'), ('fluctuating', 'NN'), ('appearances', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('markings', 'NNS'), ('they', 'PRP'), ('mapped', 'VBD'), ('so', 'RB'), ('well', 'RB'), ('.', '.'), ('All', 'PDT'), ('that', 'DT'), ('time', 'NN'), ('the', 'DT'), ('Martians', 'NNPS'), ('must', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('getting', 'VBG'), ('ready', 'JJ'), ('.', '.'), ('During', 'IN'), ('the', 'DT'), ('opposition', 'NN'), ('of', 'IN'), ('1894', 'CD'), ('a', 'DT'), ('great', 'JJ'), ('light', 'NN'), ('was', 'VBD'), ('seen', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('illuminated', 'JJ'), ('part', 'NN'), ('of', 'IN'), ('the', 'DT'), ('disk', 'NN'), (',', ','), ('first', 'RB'), ('at', 'IN'), ('the', 'DT'), ('Lick', 'NNP'), ('Observatory', 'NNP'), (',', ','), ('then', 'RB'), ('by', 'IN'), ('Perrotin', 'NNP'), ('of', 'IN'), ('Nice', 'NNP'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('by', 'IN'), ('other', 'JJ'), ('observers', 'NNS'), ('.', '.'), ('English', 'JJ'), ('readers', 'NNS'), ('heard', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('first', 'RB'), ('in', 'IN'), ('the', 'DT'), ('issue', 'NN'), ('of', 'IN'), ('Nature', 'NNP'), ('dated', 'VBD'), ('August', 'NNP'), ('2', 'CD'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Lick Observatory', 'Mars', 'Nature', 'Perrotin', 'Schiaparelli'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ne(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.17.0 requires numpy<2.0.0,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading spacy-3.8.2-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.23.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from jinja2->spacy) (3.0.1)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\envs\\mlenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.2-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 4.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/11.8 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.2/11.8 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/11.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.4/11.8 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "Downloading thinc-8.3.2-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/15.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.8/15.6 MB 4.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.4/15.6 MB 4.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.1/15.6 MB 3.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.2/15.6 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.2/15.6 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.3/15.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.3/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.4/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.0/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.7/15.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.3/15.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.3/15.6 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.6 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.0.1-cp312-cp312-win_amd64.whl (6.4 MB)\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.4 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.4 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.5/6.4 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.4 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.4/6.4 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.9/5.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, numpy, murmurhash, marisa-trie, jinja2, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, language-data, blis, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.8 jinja2-3.1.4 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.1 murmurhash-1.0.10 numpy-2.0.2 preshed-3.0.9 pydantic-2.9.2 pydantic-core-2.23.4 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 4.8 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 4.2 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 4.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shail is living in India. Sahil has 200 dollar in his account\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple Inc. is looking at buying U.K. startup for $1 billion. Barack Obama was the president of the United States.\"\n",
    "text = \"Shail is living in India. Sahil has 200 dollar in his account\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shail PERSON\n",
      "India GPE\n",
      "Sahil PERSON\n",
      "200 dollar MONEY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the entities detected in the text\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing BoW and TF-IDF in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW):\n",
      "[[0 0 1 1]\n",
      " [1 1 0 1]]\n",
      "['great' 'is' 'love' 'nlp']\n",
      "\n",
      "TF-IDF:\n",
      "[[0.         0.         0.81480247 0.57973867]\n",
      " [0.6316672  0.6316672  0.         0.44943642]]\n",
      "['great' 'is' 'love' 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example documents\n",
    "docs = [\"I love NLP\", \"NLP is great!\"]\n",
    "\n",
    "# Bag of Words (BoW) Representation\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(docs)\n",
    "print(\"Bag of Words (BoW):\")\n",
    "print(bow_matrix.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "print(\"\\nTF-IDF:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
